\section{Evaluation}

\begin{figure}[tb]
	\centering
	%\includegraphics[width=6cm]{}
	\caption{throughput comparison with and without compress}
	\label{compress}
\end{figure}

\begin{figure}[tb]
	\centering
	%\includegraphics
	\caption{cache hit and miss throughput comparison}
	\label{cache hit}
\end{figure}

\begin{figure}[tb]
	\centering
	%\includegraphics[width=6cm]{}
	\caption{throughput comparison with and without I/O burst buffer}
	\label{throughput}
\end{figure}

\begin{figure}[tb]
	\centering
	%\includegraphics[width=6cm]{}
	\caption{cost comparison with and without I/O burst buffer}
	\label{cost}
\end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[width=6cm]{compress_rate}
	\caption{compress rate}
	\label{compress rate}
\end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[width=6cm]{compress_time}
	\caption{compress time on TSUBAME V queue and AMAZON EC2}
	\label{compress time}
\end{figure}

%\begin{figure}[tb]
%	\centering
%	\includegraphics[width=6cm]{tsubamelustre}
%	\caption{TSUBAME Lustre workload}
%	\label{Lustre workload}
%\end{figure}

In this section, a simulation will be introduced based on data taking from several benchmarks on both TSUBAME V queue and AMAZON EC2 public cloud Tokyo region, using
%m3.medium instance, which has a moderate Ethernet condition and one vcpu with 3.75GiB memory, and
m3.xlarge instance, which has a high Ethernet condition and 8 vCPUs with 30GiB memory, run Amazon Linux AMI 2014.03.2(HVM) ( Fig.~\ref{throughput TSUBAME}, Fig.~\ref{throughput AMAZON to OURLAB}. Fig.~\ref{point to point connection AMAZON}, Fig.~\ref{point to point connection LAB} ).
%According to these data and definition described above, we use values defined below.

%\begin{center}
%\begin{tabular}[tb]{|c|c|}\hline
%	$D_2$&$E_1$\\\hline
%	8TB/s&1.08Gbit/s(135MB/s)\\\hline
	
%\end{tabular}
%\end{center}
For throughput between two systems and inside system, from benchmark data, it shows it is hard to achieve a high throughput with only one nodes, and also there is a limit on maximum throughput between two systems and inside system.
Although incresing nodes can increse throughput before reach the maximum, throughput achieved by each nodes decrease because of conflict.
For these reason, we use following formular for throughput between two systems and inside system.
%TSUBAME and AMAZON EC2, we use similar model in \cite{ccgrid}:
\begin{equation}
throughput=-Ax^2+Bx+C~~ A,B>0\\
\end{equation}
we use following equations to determine $A,B,C$
\begin{equation}
	\label{throughput equation}
\begin{cases}
	-A+B+C=throughput_{one}\\\nonumber
	\frac{B}{2A}=n_{max}\\\nonumber
	-An_{max}^2+Bn_{max}+C=throughput_{max}\\
\end{cases}
\end{equation}

Since it is hard for one node to fully utilize Internet and Ethernet bandwidth, according to Fig.~\ref{throughput AMAZON to OURLAB}, we assume that one node can achieve 80\% of maximum bandwidth, and by using I/O burst buffer, can achieve 100\% of maximum bandwidth of both Ethernet and Internet, here Ethernet throughput refers to Ethernet connection throughput in public cloud.

First we consider the throughput comparison between direct connection and cache hit cases by using I/O burst buffer.
If file is buffered in I/O buffer nodes, computing nodes can read it through Ethernet, Fig.~\ref{cache hit}, shows the comparison, we can see that when Ethernet throughput larger than Internet throughput, our solution can achieve a higher I/O throughput, here we assume that Ethernet throuhput can be lower than Internet throughput, but from Fig.~\ref{point to point connection AMAZON}, Fig.~\ref{throughput TSUBAME}, Ethernet usually is faster than Internet, and by using our solution can achieve a high throughput.

Then, we compare throughput with and without I/O buffer nodes.
We can see from Fig.~\ref{throughput},although our solution can be limited by both Internet and Ethernet throughput, our I/O burst buffer can fully utilize both Internet and Ethernet. Like previous comparison, when Ethernet is faster than Internet, our solution can achieve a throughput burst even file is not stored in buffer queue in I/O burst buffer like (read data from storage).

After that, we compare overall cost when use two-side buffer and one-side buffer, since execution time, in our case, I/O time and I/O throughput will affect cost, for Internet and Ethernet throughput, we use \ref{throughput equation}.
From Fig.~\ref{cost}, 
Finally, we compare throughput with and without compression, Fig.~\ref{compress rate} shows compress and Fig.~\ref{compress time} shows compress and decompress time on TSUBAME V queue and AMAZON EC2 with a different compress level by using zlib\cite{zlib}.
We can achieve a high compress rate, but the throughput is low, up to 160MB/s, depends on compress level, it is hard to find a compress library can compress data faster and smaller, so the compress throughput may become a bottleneck.
On the other hand, the compress rate is high, since the buffer size is limited, if we can make data smaller, it means we can increase the buffer hit rate, Fig.~\ref{cache hit} shows a throughput comparison between cache miss and hit.

Fig.~\ref{compress}